(dl) cnhan21@Audi:~/dl/code$ python p5c1_train.py 1 1
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
List name: /home/cnhan21/dl/BRATS2015/tumor_cropped/train_list1
Set number: 1
Number of input files: 113
Filling queue with 90 BRATS records before starting to train. This will take a few minutes.
Tensor("conv1_ot/conv1_ot:0", shape=(5, 115, 166, 129, 4), dtype=float32)
Tensor("pool1_ot:0", shape=(5, 58, 83, 65, 4), dtype=float32)
Tensor("conv2_ot/conv2_ot:0", shape=(5, 29, 42, 33, 8), dtype=float32)
Tensor("pool2_ot:0", shape=(5, 15, 21, 17, 8), dtype=float32)
Tensor("conv3_ot/conv3_ot:0", shape=(5, 15, 21, 17, 8), dtype=float32)
Tensor("conv4_ot/conv4_ot:0", shape=(5, 15, 21, 17, 8), dtype=float32)
Tensor("pool4_ot:0", shape=(5, 8, 11, 9, 8), dtype=float32)
Tensor("local5/concat:0", shape=(5, 31680), dtype=float32)
Tensor("local5/dropout/mul:0", shape=(5, 384), dtype=float32)
Tensor("local6/dropout/mul:0", shape=(5, 192), dtype=float32)
Tensor("local7/dropout/mul:0", shape=(5, 2), dtype=float32)
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.898
pciBusID 0000:02:00.0
Total memory: 7.92GiB
Free memory: 7.21GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:02:00.0)
2017-04-07 20:18:27.477316: step 0, loss = 30.97479 (5.0 examples/sec; 0.998 sec/batch)
2017-04-07 20:18:36.797849: step 10, loss = 30.91748 (5.4 examples/sec; 0.932 sec/batch)
2017-04-07 20:18:46.038592: step 20, loss = 30.85808 (5.4 examples/sec; 0.924 sec/batch)
2017-04-07 20:18:55.219248: step 30, loss = 30.93813 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 20:19:04.408588: step 40, loss = 30.70059 (5.4 examples/sec; 0.919 sec/batch)
2017-04-07 20:19:13.591286: step 50, loss = 30.97751 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 20:19:22.742045: step 60, loss = 31.00323 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 20:19:31.941055: step 70, loss = 30.76198 (5.4 examples/sec; 0.920 sec/batch)
2017-04-07 20:19:41.118974: step 80, loss = 30.67247 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 20:19:50.311973: step 90, loss = 30.79513 (5.4 examples/sec; 0.919 sec/batch)
2017-04-07 20:20:00.408146: step 100, loss = 30.91353 (5.0 examples/sec; 1.010 sec/batch)
2017-04-07 20:20:09.548529: step 110, loss = 30.68676 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 20:20:18.749777: step 120, loss = 30.54087 (5.4 examples/sec; 0.920 sec/batch)
2017-04-07 20:20:27.904173: step 130, loss = 30.48473 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 20:20:37.113196: step 140, loss = 30.61334 (5.4 examples/sec; 0.921 sec/batch)
2017-04-07 20:20:46.219483: step 150, loss = 30.58730 (5.5 examples/sec; 0.911 sec/batch)
2017-04-07 20:20:55.274000: step 160, loss = 30.56700 (5.5 examples/sec; 0.905 sec/batch)
2017-04-07 20:21:04.309052: step 170, loss = 30.61057 (5.5 examples/sec; 0.903 sec/batch)
2017-04-07 20:21:13.485477: step 180, loss = 30.33156 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 20:21:22.651006: step 190, loss = 30.57021 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 20:21:32.756860: step 200, loss = 30.20456 (4.9 examples/sec; 1.011 sec/batch)
2017-04-07 20:21:41.796496: step 210, loss = 30.66808 (5.5 examples/sec; 0.904 sec/batch)
2017-04-07 20:21:50.924684: step 220, loss = 30.30853 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 20:22:00.156315: step 230, loss = 30.55683 (5.4 examples/sec; 0.923 sec/batch)
2017-04-07 20:22:09.325622: step 240, loss = 30.33648 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 20:22:18.455009: step 250, loss = 30.33642 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 20:22:27.609167: step 260, loss = 30.43483 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 20:22:36.754198: step 270, loss = 30.45167 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 20:22:45.857864: step 280, loss = 30.17212 (5.5 examples/sec; 0.910 sec/batch)
2017-04-07 20:22:54.937820: step 290, loss = 30.27627 (5.5 examples/sec; 0.908 sec/batch)
2017-04-07 20:23:05.208334: step 300, loss = 30.59842 (4.9 examples/sec; 1.027 sec/batch)
2017-04-07 20:23:14.329225: step 310, loss = 30.28022 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 20:23:23.433321: step 320, loss = 30.25598 (5.5 examples/sec; 0.910 sec/batch)
2017-04-07 20:23:32.545332: step 330, loss = 30.12145 (5.5 examples/sec; 0.911 sec/batch)
2017-04-07 20:23:41.624841: step 340, loss = 30.27366 (5.5 examples/sec; 0.908 sec/batch)
2017-04-07 20:23:50.700923: step 350, loss = 30.33821 (5.5 examples/sec; 0.908 sec/batch)
2017-04-07 20:23:59.732769: step 360, loss = 30.07873 (5.5 examples/sec; 0.903 sec/batch)
2017-04-07 20:24:08.841080: step 370, loss = 30.00062 (5.5 examples/sec; 0.911 sec/batch)
2017-04-07 20:24:17.859790: step 380, loss = 30.12140 (5.5 examples/sec; 0.902 sec/batch)
2017-04-07 20:24:26.881083: step 390, loss = 30.20750 (5.5 examples/sec; 0.902 sec/batch)
2017-04-07 20:24:36.868820: step 400, loss = 30.04361 (5.0 examples/sec; 0.999 sec/batch)
2017-04-07 20:24:45.933435: step 410, loss = 30.02264 (5.5 examples/sec; 0.906 sec/batch)
2017-04-07 20:24:55.040935: step 420, loss = 29.93537 (5.5 examples/sec; 0.911 sec/batch)
2017-04-07 20:25:04.160772: step 430, loss = 29.79189 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 20:25:13.305609: step 440, loss = 30.20003 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 20:25:22.386817: step 450, loss = 29.91178 (5.5 examples/sec; 0.908 sec/batch)
2017-04-07 20:25:31.551865: step 460, loss = 29.88839 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 20:25:40.635413: step 470, loss = 29.82589 (5.5 examples/sec; 0.908 sec/batch)
2017-04-07 20:25:49.816054: step 480, loss = 29.74304 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 20:25:58.887776: step 490, loss = 29.72663 (5.5 examples/sec; 0.907 sec/batch)
2017-04-07 20:26:09.027648: step 500, loss = 29.80811 (4.9 examples/sec; 1.014 sec/batch)
2017-04-07 20:26:18.095257: step 510, loss = 29.59514 (5.5 examples/sec; 0.907 sec/batch)
2017-04-07 20:26:27.303105: step 520, loss = 29.62535 (5.4 examples/sec; 0.921 sec/batch)
2017-04-07 20:26:36.475811: step 530, loss = 29.65571 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 20:26:45.682596: step 540, loss = 29.60481 (5.4 examples/sec; 0.921 sec/batch)
2017-04-07 20:26:54.853171: step 550, loss = 29.79278 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 20:27:03.971472: step 560, loss = 29.73126 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 20:27:13.057990: step 570, loss = 29.67431 (5.5 examples/sec; 0.909 sec/batch)
2017-04-07 20:27:22.117610: step 580, loss = 29.64958 (5.5 examples/sec; 0.906 sec/batch)
2017-04-07 20:27:31.249006: step 590, loss = 29.52123 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 20:27:41.346312: step 600, loss = 29.29217 (5.0 examples/sec; 1.010 sec/batch)
2017-04-07 20:27:50.454225: step 610, loss = 29.45150 (5.5 examples/sec; 0.911 sec/batch)
2017-04-07 20:27:59.618391: step 620, loss = 29.50549 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 20:28:08.856641: step 630, loss = 29.33762 (5.4 examples/sec; 0.924 sec/batch)
2017-04-07 20:28:18.006238: step 640, loss = 29.44193 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 20:28:27.546913: step 650, loss = 29.32615 (5.2 examples/sec; 0.954 sec/batch)
2017-04-07 20:28:36.711001: step 660, loss = 29.82151 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 20:28:45.795518: step 670, loss = 29.40493 (5.5 examples/sec; 0.908 sec/batch)
2017-04-07 20:28:55.019049: step 680, loss = 29.20725 (5.4 examples/sec; 0.922 sec/batch)
2017-04-07 20:29:04.106869: step 690, loss = 29.38442 (5.5 examples/sec; 0.909 sec/batch)
2017-04-07 20:29:14.259873: step 700, loss = 29.25929 (4.9 examples/sec; 1.015 sec/batch)
2017-04-07 20:29:23.391923: step 710, loss = 29.26922 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 20:29:32.483671: step 720, loss = 29.38630 (5.5 examples/sec; 0.909 sec/batch)
2017-04-07 20:29:41.609997: step 730, loss = 29.25864 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 20:29:50.688072: step 740, loss = 29.28550 (5.5 examples/sec; 0.908 sec/batch)
2017-04-07 20:29:59.872646: step 750, loss = 29.06268 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 20:30:09.084303: step 760, loss = 29.04888 (5.4 examples/sec; 0.921 sec/batch)
2017-04-07 20:30:18.257338: step 770, loss = 29.17594 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 20:30:27.418336: step 780, loss = 28.96835 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 20:30:36.520912: step 790, loss = 28.97235 (5.5 examples/sec; 0.910 sec/batch)
2017-04-07 20:30:46.521160: step 800, loss = 29.06857 (5.0 examples/sec; 1.000 sec/batch)
2017-04-07 20:30:55.660643: step 810, loss = 28.83512 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 20:31:04.808160: step 820, loss = 28.96847 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 20:31:14.053855: step 830, loss = 28.91627 (5.4 examples/sec; 0.925 sec/batch)
2017-04-07 20:31:23.222707: step 840, loss = 28.99184 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 20:31:32.360872: step 850, loss = 28.87416 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 20:31:41.529452: step 860, loss = 28.82931 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 20:31:50.696369: step 870, loss = 28.95612 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 20:31:59.788734: step 880, loss = 28.69144 (5.5 examples/sec; 0.909 sec/batch)
2017-04-07 20:32:08.936377: step 890, loss = 29.02390 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 20:32:19.106905: step 900, loss = 28.68196 (4.9 examples/sec; 1.017 sec/batch)
2017-04-07 20:32:28.316146: step 910, loss = 28.61884 (5.4 examples/sec; 0.921 sec/batch)
2017-04-07 20:32:37.433405: step 920, loss = 28.74122 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 20:32:46.573548: step 930, loss = 28.75463 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 20:32:55.729606: step 940, loss = 28.79369 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 20:33:04.806595: step 950, loss = 28.67446 (5.5 examples/sec; 0.908 sec/batch)
2017-04-07 20:33:13.945165: step 960, loss = 28.69391 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 20:33:23.172336: step 970, loss = 28.72752 (5.4 examples/sec; 0.923 sec/batch)
2017-04-07 20:33:32.309103: step 980, loss = 28.66987 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 20:33:41.374325: step 990, loss = 28.73896 (5.5 examples/sec; 0.907 sec/batch)
2017-04-07 20:33:51.551470: step 1000, loss = 28.39248 (4.9 examples/sec; 1.018 sec/batch)
2017-04-07 20:34:00.678698: step 1010, loss = 28.58332 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 20:34:09.875586: step 1020, loss = 28.18431 (5.4 examples/sec; 0.920 sec/batch)
2017-04-07 20:34:19.033156: step 1030, loss = 28.38445 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 20:34:28.307486: step 1040, loss = 28.43061 (5.4 examples/sec; 0.927 sec/batch)
2017-04-07 20:34:37.473048: step 1050, loss = 28.46217 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 20:34:46.582056: step 1060, loss = 28.51317 (5.5 examples/sec; 0.911 sec/batch)
2017-04-07 20:34:55.738955: step 1070, loss = 28.52757 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 20:35:04.890487: step 1080, loss = 28.61791 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 20:35:13.996310: step 1090, loss = 28.26346 (5.5 examples/sec; 0.911 sec/batch)
2017-04-07 20:35:24.171079: step 1100, loss = 28.45589 (4.9 examples/sec; 1.017 sec/batch)
2017-04-07 20:35:33.352616: step 1110, loss = 28.08466 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 20:35:42.497844: step 1120, loss = 28.62692 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 20:35:51.798581: step 1130, loss = 28.41932 (5.4 examples/sec; 0.930 sec/batch)
2017-04-07 20:36:00.955519: step 1140, loss = 28.24399 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 20:36:10.108569: step 1150, loss = 28.33400 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 20:36:19.240208: step 1160, loss = 28.56523 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 20:36:28.328323: step 1170, loss = 28.12009 (5.5 examples/sec; 0.909 sec/batch)
2017-04-07 20:36:37.446573: step 1180, loss = 28.15129 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 20:36:46.561518: step 1190, loss = 28.32853 (5.5 examples/sec; 0.911 sec/batch)
2017-04-07 20:36:56.729451: step 1200, loss = 28.36318 (4.9 examples/sec; 1.017 sec/batch)
2017-04-07 20:37:05.866448: step 1210, loss = 28.23594 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 20:37:14.976703: step 1220, loss = 27.98731 (5.5 examples/sec; 0.911 sec/batch)
2017-04-07 20:37:24.210401: step 1230, loss = 27.91895 (5.4 examples/sec; 0.923 sec/batch)
2017-04-07 20:37:33.357259: step 1240, loss = 27.86703 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 20:37:42.454852: step 1250, loss = 27.97890 (5.5 examples/sec; 0.910 sec/batch)
2017-04-07 20:37:51.560768: step 1260, loss = 27.82036 (5.5 examples/sec; 0.911 sec/batch)
2017-04-07 20:38:00.705010: step 1270, loss = 27.74330 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 20:38:09.851695: step 1280, loss = 27.75639 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 20:38:18.994660: step 1290, loss = 28.01271 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 20:38:29.716981: step 1300, loss = 27.77747 (4.7 examples/sec; 1.072 sec/batch)
2017-04-07 20:38:38.801055: step 1310, loss = 27.76143 (5.5 examples/sec; 0.908 sec/batch)
2017-04-07 20:38:47.962188: step 1320, loss = 27.73336 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 20:38:57.107853: step 1330, loss = 27.81124 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 20:39:06.210588: step 1340, loss = 28.05416 (5.5 examples/sec; 0.910 sec/batch)
2017-04-07 20:39:15.346517: step 1350, loss = 27.81181 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 20:39:24.478147: step 1360, loss = 27.91653 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 20:39:33.652823: step 1370, loss = 27.94197 (5.4 examples/sec; 0.917 sec/batch)
2017-04-07 20:39:42.771808: step 1380, loss = 27.71803 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 20:39:51.992461: step 1390, loss = 27.43874 (5.4 examples/sec; 0.922 sec/batch)
2017-04-07 20:40:02.084359: step 1400, loss = 27.81265 (5.0 examples/sec; 1.009 sec/batch)
2017-04-07 20:40:11.338940: step 1410, loss = 27.48405 (5.4 examples/sec; 0.925 sec/batch)
2017-04-07 20:40:20.612453: step 1420, loss = 27.46772 (5.4 examples/sec; 0.927 sec/batch)
2017-04-07 20:40:29.813992: step 1430, loss = 27.65585 (5.4 examples/sec; 0.920 sec/batch)
2017-04-07 20:40:38.978089: step 1440, loss = 27.45338 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 20:40:48.219910: step 1450, loss = 27.33167 (5.4 examples/sec; 0.924 sec/batch)
2017-04-07 20:40:57.521247: step 1460, loss = 27.03692 (5.4 examples/sec; 0.930 sec/batch)
2017-04-07 20:41:06.732758: step 1470, loss = 27.40750 (5.4 examples/sec; 0.921 sec/batch)
2017-04-07 20:41:15.915610: step 1480, loss = 27.26711 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 20:41:25.109717: step 1490, loss = 27.12886 (5.4 examples/sec; 0.919 sec/batch)
2017-04-07 20:41:35.179665: step 1500, loss = 27.22915 (5.0 examples/sec; 1.007 sec/batch)
2017-04-07 20:41:44.376717: step 1510, loss = 27.58555 (5.4 examples/sec; 0.920 sec/batch)
2017-04-07 20:41:53.604335: step 1520, loss = 27.27378 (5.4 examples/sec; 0.923 sec/batch)
2017-04-07 20:42:02.775697: step 1530, loss = 27.48774 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 20:42:12.006932: step 1540, loss = 27.71044 (5.4 examples/sec; 0.923 sec/batch)
2017-04-07 20:42:21.136847: step 1550, loss = 27.09625 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 20:42:30.406017: step 1560, loss = 27.24841 (5.4 examples/sec; 0.927 sec/batch)
2017-04-07 20:42:39.630171: step 1570, loss = 27.14186 (5.4 examples/sec; 0.922 sec/batch)
2017-04-07 20:42:48.793513: step 1580, loss = 27.69633 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 20:42:57.999997: step 1590, loss = 27.50438 (5.4 examples/sec; 0.921 sec/batch)
2017-04-07 20:43:08.116375: step 1600, loss = 27.04734 (4.9 examples/sec; 1.012 sec/batch)
2017-04-07 20:43:17.279609: step 1610, loss = 27.09256 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 20:43:26.466868: step 1620, loss = 26.75592 (5.4 examples/sec; 0.919 sec/batch)
2017-04-07 20:43:35.645863: step 1630, loss = 27.04079 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 20:43:44.781047: step 1640, loss = 27.13937 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 20:43:54.017544: step 1650, loss = 26.79051 (5.4 examples/sec; 0.924 sec/batch)
2017-04-07 20:44:03.234942: step 1660, loss = 27.08073 (5.4 examples/sec; 0.922 sec/batch)
2017-04-07 20:44:12.410440: step 1670, loss = 26.71990 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 20:44:21.713093: step 1680, loss = 26.76694 (5.4 examples/sec; 0.930 sec/batch)
2017-04-07 20:44:30.906295: step 1690, loss = 26.92528 (5.4 examples/sec; 0.919 sec/batch)
2017-04-07 20:44:41.010275: step 1700, loss = 27.04027 (4.9 examples/sec; 1.010 sec/batch)
2017-04-07 20:44:50.275343: step 1710, loss = 26.93539 (5.4 examples/sec; 0.927 sec/batch)
2017-04-07 20:44:59.502811: step 1720, loss = 26.54584 (5.4 examples/sec; 0.923 sec/batch)
2017-04-07 20:45:08.713728: step 1730, loss = 26.88894 (5.4 examples/sec; 0.921 sec/batch)
2017-04-07 20:45:17.858020: step 1740, loss = 26.89248 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 20:45:26.992897: step 1750, loss = 27.20947 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 20:45:36.193554: step 1760, loss = 26.79266 (5.4 examples/sec; 0.920 sec/batch)
2017-04-07 20:45:45.299539: step 1770, loss = 26.61617 (5.5 examples/sec; 0.911 sec/batch)
2017-04-07 20:45:54.641014: step 1780, loss = 26.36790 (5.4 examples/sec; 0.934 sec/batch)
2017-04-07 20:46:03.819134: step 1790, loss = 26.58594 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 20:46:13.874650: step 1800, loss = 26.67707 (5.0 examples/sec; 1.006 sec/batch)
2017-04-07 20:46:23.031500: step 1810, loss = 26.71923 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 20:46:32.209084: step 1820, loss = 26.64394 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 20:46:41.420364: step 1830, loss = 26.51421 (5.4 examples/sec; 0.921 sec/batch)
2017-04-07 20:46:50.644663: step 1840, loss = 26.70669 (5.4 examples/sec; 0.922 sec/batch)
2017-04-07 20:46:59.795865: step 1850, loss = 26.46125 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 20:47:08.957999: step 1860, loss = 26.52518 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 20:47:18.135511: step 1870, loss = 26.79974 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 20:47:27.281139: step 1880, loss = 26.54640 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 20:47:36.463581: step 1890, loss = 26.62751 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 20:47:46.588015: step 1900, loss = 26.51681 (4.9 examples/sec; 1.012 sec/batch)
2017-04-07 20:47:55.811863: step 1910, loss = 26.25240 (5.4 examples/sec; 0.922 sec/batch)
2017-04-07 20:48:05.082294: step 1920, loss = 26.32702 (5.4 examples/sec; 0.927 sec/batch)
2017-04-07 20:48:14.316433: step 1930, loss = 26.50142 (5.4 examples/sec; 0.923 sec/batch)
2017-04-07 20:48:23.449332: step 1940, loss = 27.06858 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 20:48:33.179641: step 1950, loss = 26.31975 (5.1 examples/sec; 0.973 sec/batch)
2017-04-07 20:48:42.350631: step 1960, loss = 26.20121 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 20:48:51.532608: step 1970, loss = 26.28247 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 20:49:00.652314: step 1980, loss = 26.22803 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 20:49:09.815872: step 1990, loss = 26.25905 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 20:49:19.947622: step 2000, loss = 26.13709 (4.9 examples/sec; 1.013 sec/batch)
2017-04-07 20:49:29.086195: step 2010, loss = 26.15537 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 20:49:38.207304: step 2020, loss = 27.02011 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 20:49:47.442498: step 2030, loss = 26.05398 (5.4 examples/sec; 0.924 sec/batch)
2017-04-07 20:49:56.636354: step 2040, loss = 26.53380 (5.4 examples/sec; 0.919 sec/batch)
2017-04-07 20:50:05.827696: step 2050, loss = 25.95046 (5.4 examples/sec; 0.919 sec/batch)
2017-04-07 20:50:15.109210: step 2060, loss = 25.98079 (5.4 examples/sec; 0.928 sec/batch)
2017-04-07 20:50:24.228167: step 2070, loss = 26.36579 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 20:50:33.400395: step 2080, loss = 26.21903 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 20:50:42.588371: step 2090, loss = 25.83986 (5.4 examples/sec; 0.919 sec/batch)
2017-04-07 20:50:52.679648: step 2100, loss = 25.88239 (5.0 examples/sec; 1.009 sec/batch)
2017-04-07 20:51:01.878953: step 2110, loss = 25.65295 (5.4 examples/sec; 0.920 sec/batch)
2017-04-07 20:51:11.055610: step 2120, loss = 25.72128 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 20:51:20.213536: step 2130, loss = 25.81981 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 20:51:29.364202: step 2140, loss = 25.54539 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 20:51:38.572127: step 2150, loss = 25.84616 (5.4 examples/sec; 0.921 sec/batch)
2017-04-07 20:51:47.815897: step 2160, loss = 25.77218 (5.4 examples/sec; 0.924 sec/batch)
2017-04-07 20:51:56.996457: step 2170, loss = 26.28431 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 20:52:06.167775: step 2180, loss = 25.79245 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 20:52:15.336486: step 2190, loss = 25.84529 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 20:52:25.469025: step 2200, loss = 25.69689 (4.9 examples/sec; 1.013 sec/batch)
2017-04-07 20:52:34.704548: step 2210, loss = 25.60686 (5.4 examples/sec; 0.924 sec/batch)
2017-04-07 20:52:43.809502: step 2220, loss = 25.91821 (5.5 examples/sec; 0.910 sec/batch)
2017-04-07 20:52:52.988449: step 2230, loss = 25.85386 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 20:53:02.164371: step 2240, loss = 25.60743 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 20:53:11.383039: step 2250, loss = 25.60369 (5.4 examples/sec; 0.922 sec/batch)
2017-04-07 20:53:20.563502: step 2260, loss = 25.75681 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 20:53:29.750480: step 2270, loss = 25.75752 (5.4 examples/sec; 0.919 sec/batch)
2017-04-07 20:53:38.994707: step 2280, loss = 25.53911 (5.4 examples/sec; 0.924 sec/batch)
2017-04-07 20:53:48.205527: step 2290, loss = 25.41563 (5.4 examples/sec; 0.921 sec/batch)
2017-04-07 20:53:58.451228: step 2300, loss = 25.37357 (4.9 examples/sec; 1.025 sec/batch)
2017-04-07 20:54:07.576470: step 2310, loss = 25.58566 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 20:54:16.714923: step 2320, loss = 25.23403 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 20:54:25.912889: step 2330, loss = 25.29291 (5.4 examples/sec; 0.920 sec/batch)
2017-04-07 20:54:35.132427: step 2340, loss = 25.33006 (5.4 examples/sec; 0.922 sec/batch)
2017-04-07 20:54:44.295192: step 2350, loss = 25.57922 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 20:54:53.480433: step 2360, loss = 25.10349 (5.4 examples/sec; 0.919 sec/batch)
2017-04-07 20:55:02.667386: step 2370, loss = 25.26233 (5.4 examples/sec; 0.919 sec/batch)
2017-04-07 20:55:11.869097: step 2380, loss = 25.37555 (5.4 examples/sec; 0.920 sec/batch)
2017-04-07 20:55:21.018794: step 2390, loss = 25.59457 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 20:55:31.104634: step 2400, loss = 25.42775 (5.0 examples/sec; 1.009 sec/batch)
2017-04-07 20:55:40.277739: step 2410, loss = 25.53726 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 20:55:49.419575: step 2420, loss = 25.38800 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 20:55:58.584680: step 2430, loss = 24.98542 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 20:56:07.761502: step 2440, loss = 25.30434 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 20:56:16.920994: step 2450, loss = 25.43793 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 20:56:26.066760: step 2460, loss = 25.46569 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 20:56:35.245191: step 2470, loss = 25.05134 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 20:56:44.414709: step 2480, loss = 25.06923 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 20:56:53.556642: step 2490, loss = 24.89343 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 20:57:03.742363: step 2500, loss = 24.87588 (4.9 examples/sec; 1.019 sec/batch)
2017-04-07 20:57:12.867108: step 2510, loss = 24.94871 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 20:57:22.006252: step 2520, loss = 25.14741 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 20:57:31.161480: step 2530, loss = 24.84901 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 20:57:40.332632: step 2540, loss = 25.07960 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 20:57:49.463217: step 2550, loss = 24.96870 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 20:57:58.632205: step 2560, loss = 24.74500 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 20:58:07.792807: step 2570, loss = 24.95448 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 20:58:16.967782: step 2580, loss = 24.79079 (5.4 examples/sec; 0.917 sec/batch)
2017-04-07 20:58:26.112122: step 2590, loss = 24.84427 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 20:58:36.797853: step 2600, loss = 24.92733 (4.7 examples/sec; 1.069 sec/batch)
2017-04-07 20:58:45.953779: step 2610, loss = 24.99806 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 20:58:55.101153: step 2620, loss = 24.85148 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 20:59:04.240816: step 2630, loss = 24.83122 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 20:59:13.414053: step 2640, loss = 24.97162 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 20:59:22.597996: step 2650, loss = 24.65347 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 20:59:31.779499: step 2660, loss = 24.77391 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 20:59:40.933755: step 2670, loss = 24.90950 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 20:59:50.061716: step 2680, loss = 24.64804 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 20:59:59.215779: step 2690, loss = 24.57806 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 21:00:09.214115: step 2700, loss = 24.71712 (5.0 examples/sec; 1.000 sec/batch)
2017-04-07 21:00:18.367680: step 2710, loss = 24.81179 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 21:00:27.585350: step 2720, loss = 24.52107 (5.4 examples/sec; 0.922 sec/batch)
2017-04-07 21:00:36.777696: step 2730, loss = 24.64801 (5.4 examples/sec; 0.919 sec/batch)
2017-04-07 21:00:45.935741: step 2740, loss = 24.61910 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 21:00:55.080513: step 2750, loss = 24.60125 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 21:01:04.284252: step 2760, loss = 24.76458 (5.4 examples/sec; 0.920 sec/batch)
2017-04-07 21:01:13.501238: step 2770, loss = 24.28799 (5.4 examples/sec; 0.922 sec/batch)
2017-04-07 21:01:22.689742: step 2780, loss = 24.55956 (5.4 examples/sec; 0.919 sec/batch)
2017-04-07 21:01:31.835207: step 2790, loss = 24.65930 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 21:01:42.011781: step 2800, loss = 24.37301 (4.9 examples/sec; 1.018 sec/batch)
2017-04-07 21:01:51.136677: step 2810, loss = 24.50524 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 21:02:00.274673: step 2820, loss = 24.66492 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 21:02:09.474653: step 2830, loss = 24.37629 (5.4 examples/sec; 0.920 sec/batch)
2017-04-07 21:02:18.655797: step 2840, loss = 24.42589 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 21:02:27.869767: step 2850, loss = 24.12819 (5.4 examples/sec; 0.921 sec/batch)
2017-04-07 21:02:37.037033: step 2860, loss = 24.31392 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 21:02:46.218672: step 2870, loss = 24.23256 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 21:02:55.392875: step 2880, loss = 24.28822 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 21:03:04.526219: step 2890, loss = 24.23279 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 21:03:14.665306: step 2900, loss = 24.48914 (4.9 examples/sec; 1.014 sec/batch)
2017-04-07 21:03:23.791181: step 2910, loss = 24.15283 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 21:03:32.990859: step 2920, loss = 24.14315 (5.4 examples/sec; 0.920 sec/batch)
2017-04-07 21:03:42.160759: step 2930, loss = 24.39706 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 21:03:51.319831: step 2940, loss = 24.25398 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 21:04:00.516285: step 2950, loss = 24.35398 (5.4 examples/sec; 0.920 sec/batch)
2017-04-07 21:04:09.634621: step 2960, loss = 24.44736 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 21:04:18.791800: step 2970, loss = 24.20905 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 21:04:27.974190: step 2980, loss = 24.16684 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 21:04:37.161265: step 2990, loss = 23.86099 (5.4 examples/sec; 0.919 sec/batch)
2017-04-07 21:04:47.243040: step 3000, loss = 23.86624 (5.0 examples/sec; 1.008 sec/batch)
2017-04-07 21:04:56.357478: step 3010, loss = 23.96077 (5.5 examples/sec; 0.911 sec/batch)
2017-04-07 21:05:05.516135: step 3020, loss = 24.25730 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 21:05:14.663007: step 3030, loss = 24.33866 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 21:05:23.782177: step 3040, loss = 24.01951 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 21:05:33.007246: step 3050, loss = 23.82867 (5.4 examples/sec; 0.923 sec/batch)
2017-04-07 21:05:42.242624: step 3060, loss = 23.86604 (5.4 examples/sec; 0.924 sec/batch)
2017-04-07 21:05:51.417067: step 3070, loss = 23.71009 (5.4 examples/sec; 0.917 sec/batch)
2017-04-07 21:06:00.559358: step 3080, loss = 23.97074 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 21:06:09.711889: step 3090, loss = 23.80907 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 21:06:19.867912: step 3100, loss = 23.65203 (4.9 examples/sec; 1.016 sec/batch)
2017-04-07 21:06:28.999341: step 3110, loss = 23.90999 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 21:06:38.156328: step 3120, loss = 23.89128 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 21:06:47.314296: step 3130, loss = 23.67326 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 21:06:56.418695: step 3140, loss = 23.71589 (5.5 examples/sec; 0.910 sec/batch)
2017-04-07 21:07:05.552910: step 3150, loss = 23.98566 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 21:07:14.728910: step 3160, loss = 23.82128 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 21:07:23.874756: step 3170, loss = 23.65927 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 21:07:32.999255: step 3180, loss = 23.50079 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 21:07:42.122919: step 3190, loss = 24.03652 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 21:07:52.224067: step 3200, loss = 23.49012 (4.9 examples/sec; 1.010 sec/batch)
2017-04-07 21:08:01.376573: step 3210, loss = 23.72182 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 21:08:10.552153: step 3220, loss = 23.89684 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 21:08:19.685720: step 3230, loss = 23.69075 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 21:08:29.509985: step 3240, loss = 23.38838 (5.1 examples/sec; 0.982 sec/batch)
2017-04-07 21:08:38.648553: step 3250, loss = 23.38098 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 21:08:47.806130: step 3260, loss = 23.39200 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 21:08:56.917611: step 3270, loss = 23.47191 (5.5 examples/sec; 0.911 sec/batch)
2017-04-07 21:09:06.091907: step 3280, loss = 23.61647 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 21:09:15.263392: step 3290, loss = 23.29542 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 21:09:25.486838: step 3300, loss = 23.41741 (4.9 examples/sec; 1.022 sec/batch)
2017-04-07 21:09:34.609198: step 3310, loss = 23.26496 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 21:09:43.763991: step 3320, loss = 23.24943 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 21:09:52.940792: step 3330, loss = 23.49801 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 21:10:02.095180: step 3340, loss = 23.38840 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 21:10:11.213087: step 3350, loss = 23.49169 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 21:10:20.430634: step 3360, loss = 23.30607 (5.4 examples/sec; 0.922 sec/batch)
2017-04-07 21:10:29.551164: step 3370, loss = 23.40347 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 21:10:38.699881: step 3380, loss = 23.14337 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 21:10:47.897206: step 3390, loss = 23.25356 (5.4 examples/sec; 0.920 sec/batch)
2017-04-07 21:10:57.972417: step 3400, loss = 23.37248 (5.0 examples/sec; 1.008 sec/batch)
2017-04-07 21:11:07.147720: step 3410, loss = 23.29451 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 21:11:16.332355: step 3420, loss = 23.24222 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 21:11:25.448260: step 3430, loss = 23.34974 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 21:11:34.625707: step 3440, loss = 23.24710 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 21:11:43.778776: step 3450, loss = 23.02112 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 21:11:52.968109: step 3460, loss = 23.12808 (5.4 examples/sec; 0.919 sec/batch)
2017-04-07 21:12:02.101044: step 3470, loss = 23.52110 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 21:12:11.216401: step 3480, loss = 23.09229 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 21:12:20.326575: step 3490, loss = 23.06661 (5.5 examples/sec; 0.911 sec/batch)
2017-04-07 21:12:30.619309: step 3500, loss = 23.07086 (4.9 examples/sec; 1.029 sec/batch)
2017-04-07 21:12:39.842853: step 3510, loss = 23.03069 (5.4 examples/sec; 0.922 sec/batch)
2017-04-07 21:12:49.022617: step 3520, loss = 23.42764 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 21:12:58.158962: step 3530, loss = 23.14709 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 21:13:07.380554: step 3540, loss = 23.06941 (5.4 examples/sec; 0.922 sec/batch)
2017-04-07 21:13:16.578038: step 3550, loss = 22.98344 (5.4 examples/sec; 0.920 sec/batch)
2017-04-07 21:13:25.784558: step 3560, loss = 23.07740 (5.4 examples/sec; 0.921 sec/batch)
2017-04-07 21:13:34.985244: step 3570, loss = 22.92583 (5.4 examples/sec; 0.920 sec/batch)
2017-04-07 21:13:44.132578: step 3580, loss = 22.90344 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 21:13:53.280169: step 3590, loss = 22.88511 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 21:14:03.340198: step 3600, loss = 22.86619 (5.0 examples/sec; 1.006 sec/batch)
2017-04-07 21:14:12.487531: step 3610, loss = 22.85125 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 21:14:21.616841: step 3620, loss = 22.82973 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 21:14:30.776025: step 3630, loss = 22.81337 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 21:14:39.943209: step 3640, loss = 22.77521 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 21:14:49.109035: step 3650, loss = 22.64756 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 21:14:58.284959: step 3660, loss = 22.76489 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 21:15:07.462335: step 3670, loss = 23.02888 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 21:15:16.651833: step 3680, loss = 22.72542 (5.4 examples/sec; 0.919 sec/batch)
2017-04-07 21:15:25.857910: step 3690, loss = 22.70470 (5.4 examples/sec; 0.921 sec/batch)
2017-04-07 21:15:36.067308: step 3700, loss = 22.82726 (4.9 examples/sec; 1.021 sec/batch)
2017-04-07 21:15:45.175006: step 3710, loss = 23.01983 (5.5 examples/sec; 0.911 sec/batch)
2017-04-07 21:15:54.338526: step 3720, loss = 22.52338 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 21:16:03.542901: step 3730, loss = 22.91129 (5.4 examples/sec; 0.920 sec/batch)
2017-04-07 21:16:12.742988: step 3740, loss = 22.50712 (5.4 examples/sec; 0.920 sec/batch)
2017-04-07 21:16:21.917471: step 3750, loss = 22.47473 (5.4 examples/sec; 0.917 sec/batch)
2017-04-07 21:16:31.061125: step 3760, loss = 22.57931 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 21:16:40.230789: step 3770, loss = 22.44022 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 21:16:49.372042: step 3780, loss = 22.54552 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 21:16:58.541639: step 3790, loss = 22.80375 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 21:17:08.799232: step 3800, loss = 22.37652 (4.9 examples/sec; 1.026 sec/batch)
2017-04-07 21:17:17.973044: step 3810, loss = 22.78114 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 21:17:27.166030: step 3820, loss = 22.37837 (5.4 examples/sec; 0.919 sec/batch)
2017-04-07 21:17:36.298092: step 3830, loss = 22.84221 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 21:17:45.444555: step 3840, loss = 22.57611 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 21:17:54.588747: step 3850, loss = 22.42233 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 21:18:03.695902: step 3860, loss = 22.67825 (5.5 examples/sec; 0.911 sec/batch)
2017-04-07 21:18:12.855918: step 3870, loss = 22.24480 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 21:18:21.993343: step 3880, loss = 22.22945 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 21:18:31.633082: step 3890, loss = 22.76353 (5.2 examples/sec; 0.964 sec/batch)
2017-04-07 21:18:41.777324: step 3900, loss = 22.33054 (4.9 examples/sec; 1.014 sec/batch)
2017-04-07 21:18:50.910618: step 3910, loss = 22.31416 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 21:19:00.046908: step 3920, loss = 22.29503 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 21:19:09.223573: step 3930, loss = 22.31539 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 21:19:18.324994: step 3940, loss = 22.39910 (5.5 examples/sec; 0.910 sec/batch)
2017-04-07 21:19:27.486932: step 3950, loss = 22.10476 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 21:19:36.672408: step 3960, loss = 22.77832 (5.4 examples/sec; 0.919 sec/batch)
2017-04-07 21:19:45.816340: step 3970, loss = 22.20716 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 21:19:54.995002: step 3980, loss = 22.18869 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 21:20:04.112664: step 3990, loss = 22.17099 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 21:20:14.095343: step 4000, loss = 22.01904 (5.0 examples/sec; 0.998 sec/batch)
2017-04-07 21:20:23.069769: step 4010, loss = 22.27438 (5.6 examples/sec; 0.897 sec/batch)
2017-04-07 21:20:31.999649: step 4020, loss = 22.25701 (5.6 examples/sec; 0.893 sec/batch)
2017-04-07 21:20:40.959552: step 4030, loss = 22.10053 (5.6 examples/sec; 0.896 sec/batch)
2017-04-07 21:20:49.887867: step 4040, loss = 21.94456 (5.6 examples/sec; 0.893 sec/batch)
2017-04-07 21:20:59.022596: step 4050, loss = 21.92686 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 21:21:08.158304: step 4060, loss = 21.90928 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 21:21:17.319959: step 4070, loss = 21.89356 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 21:21:26.488235: step 4080, loss = 21.87432 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 21:21:35.661498: step 4090, loss = 22.27269 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 21:21:45.716485: step 4100, loss = 21.97835 (5.0 examples/sec; 1.005 sec/batch)
2017-04-07 21:21:54.895982: step 4110, loss = 21.96053 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 21:22:04.015292: step 4120, loss = 24.18727 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 21:22:13.019307: step 4130, loss = 22.32620 (5.6 examples/sec; 0.900 sec/batch)
2017-04-07 21:22:22.005262: step 4140, loss = 22.65119 (5.6 examples/sec; 0.899 sec/batch)
2017-04-07 21:22:30.998539: step 4150, loss = 22.08892 (5.6 examples/sec; 0.899 sec/batch)
2017-04-07 21:22:39.944222: step 4160, loss = 22.19688 (5.6 examples/sec; 0.895 sec/batch)
2017-04-07 21:22:48.911919: step 4170, loss = 21.97615 (5.6 examples/sec; 0.897 sec/batch)
2017-04-07 21:22:58.048886: step 4180, loss = 22.21788 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 21:23:07.204584: step 4190, loss = 22.03862 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 21:23:17.353341: step 4200, loss = 21.95368 (4.9 examples/sec; 1.015 sec/batch)
2017-04-07 21:23:26.529447: step 4210, loss = 21.98416 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 21:23:35.715701: step 4220, loss = 21.78435 (5.4 examples/sec; 0.919 sec/batch)
2017-04-07 21:23:44.894306: step 4230, loss = 21.65484 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 21:23:54.071527: step 4240, loss = 22.15901 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 21:24:03.236148: step 4250, loss = 21.87713 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 21:24:12.259244: step 4260, loss = 21.84846 (5.5 examples/sec; 0.902 sec/batch)
2017-04-07 21:24:21.185494: step 4270, loss = 21.99715 (5.6 examples/sec; 0.893 sec/batch)
2017-04-07 21:24:30.095707: step 4280, loss = 21.81379 (5.6 examples/sec; 0.891 sec/batch)
2017-04-07 21:24:38.998268: step 4290, loss = 21.69168 (5.6 examples/sec; 0.890 sec/batch)
2017-04-07 21:24:48.849523: step 4300, loss = 21.80608 (5.1 examples/sec; 0.985 sec/batch)
2017-04-07 21:24:58.021076: step 4310, loss = 22.17663 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 21:25:07.253683: step 4320, loss = 21.62001 (5.4 examples/sec; 0.923 sec/batch)
2017-04-07 21:25:16.421025: step 4330, loss = 21.72812 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 21:25:25.606485: step 4340, loss = 21.62924 (5.4 examples/sec; 0.919 sec/batch)
2017-04-07 21:25:34.803316: step 4350, loss = 21.56317 (5.4 examples/sec; 0.920 sec/batch)
2017-04-07 21:25:43.988823: step 4360, loss = 21.54880 (5.4 examples/sec; 0.919 sec/batch)
2017-04-07 21:25:53.165412: step 4370, loss = 21.51965 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 21:26:02.351622: step 4380, loss = 21.71264 (5.4 examples/sec; 0.919 sec/batch)
2017-04-07 21:26:11.379777: step 4390, loss = 21.76329 (5.5 examples/sec; 0.903 sec/batch)
2017-04-07 21:26:21.258410: step 4400, loss = 21.65709 (5.1 examples/sec; 0.988 sec/batch)
2017-04-07 21:26:30.283049: step 4410, loss = 21.49627 (5.5 examples/sec; 0.902 sec/batch)
2017-04-07 21:26:39.241397: step 4420, loss = 21.45020 (5.6 examples/sec; 0.896 sec/batch)
2017-04-07 21:26:48.236581: step 4430, loss = 21.58318 (5.6 examples/sec; 0.900 sec/batch)
2017-04-07 21:26:57.370880: step 4440, loss = 21.40319 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 21:27:06.597165: step 4450, loss = 21.52266 (5.4 examples/sec; 0.923 sec/batch)
2017-04-07 21:27:15.830419: step 4460, loss = 21.50634 (5.4 examples/sec; 0.923 sec/batch)
2017-04-07 21:27:25.011516: step 4470, loss = 21.21395 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 21:27:34.162987: step 4480, loss = 21.74885 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 21:27:43.344218: step 4490, loss = 21.45970 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 21:27:53.648810: step 4500, loss = 21.57821 (4.9 examples/sec; 1.030 sec/batch)
2017-04-07 21:28:02.763170: step 4510, loss = 21.55980 (5.5 examples/sec; 0.911 sec/batch)
2017-04-07 21:28:11.807427: step 4520, loss = 21.13442 (5.5 examples/sec; 0.904 sec/batch)
2017-04-07 21:28:20.759690: step 4530, loss = 21.10984 (5.6 examples/sec; 0.895 sec/batch)
2017-04-07 21:28:30.305075: step 4540, loss = 21.37073 (5.2 examples/sec; 0.955 sec/batch)
2017-04-07 21:28:39.249138: step 4550, loss = 21.22363 (5.6 examples/sec; 0.894 sec/batch)
2017-04-07 21:28:48.201484: step 4560, loss = 21.06088 (5.6 examples/sec; 0.895 sec/batch)
2017-04-07 21:28:57.325205: step 4570, loss = 21.19005 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 21:29:06.497912: step 4580, loss = 21.02572 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 21:29:15.750853: step 4590, loss = 21.14946 (5.4 examples/sec; 0.925 sec/batch)
2017-04-07 21:29:25.870881: step 4600, loss = 21.27092 (4.9 examples/sec; 1.012 sec/batch)
2017-04-07 21:29:35.009947: step 4610, loss = 21.11676 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 21:29:44.170890: step 4620, loss = 21.09700 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 21:29:53.360162: step 4630, loss = 21.22048 (5.4 examples/sec; 0.919 sec/batch)
2017-04-07 21:30:02.577753: step 4640, loss = 21.06354 (5.4 examples/sec; 0.922 sec/batch)
2017-04-07 21:30:11.747941: step 4650, loss = 20.91108 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 21:30:20.861715: step 4660, loss = 21.03777 (5.5 examples/sec; 0.911 sec/batch)
2017-04-07 21:30:30.005583: step 4670, loss = 21.01380 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 21:30:39.128167: step 4680, loss = 21.27397 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 21:30:48.218474: step 4690, loss = 20.98188 (5.5 examples/sec; 0.909 sec/batch)
2017-04-07 21:30:58.322170: step 4700, loss = 20.90446 (4.9 examples/sec; 1.010 sec/batch)
2017-04-07 21:31:07.508979: step 4710, loss = 21.08589 (5.4 examples/sec; 0.919 sec/batch)
2017-04-07 21:31:16.648299: step 4720, loss = 21.07139 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 21:31:25.763662: step 4730, loss = 20.92038 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 21:31:34.862218: step 4740, loss = 21.03552 (5.5 examples/sec; 0.910 sec/batch)
2017-04-07 21:31:43.976476: step 4750, loss = 20.88037 (5.5 examples/sec; 0.911 sec/batch)
2017-04-07 21:31:53.074403: step 4760, loss = 21.00234 (5.5 examples/sec; 0.910 sec/batch)
2017-04-07 21:32:02.163704: step 4770, loss = 21.16916 (5.5 examples/sec; 0.909 sec/batch)
2017-04-07 21:32:11.234592: step 4780, loss = 20.83075 (5.5 examples/sec; 0.907 sec/batch)
2017-04-07 21:32:20.304614: step 4790, loss = 20.81423 (5.5 examples/sec; 0.907 sec/batch)
2017-04-07 21:32:30.466700: step 4800, loss = 21.07479 (4.9 examples/sec; 1.016 sec/batch)
2017-04-07 21:32:39.560488: step 4810, loss = 21.05830 (5.5 examples/sec; 0.909 sec/batch)
2017-04-07 21:32:48.693308: step 4820, loss = 20.62592 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 21:32:57.773973: step 4830, loss = 20.74815 (5.5 examples/sec; 0.908 sec/batch)
2017-04-07 21:33:06.902003: step 4840, loss = 20.73848 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 21:33:16.002742: step 4850, loss = 20.71508 (5.5 examples/sec; 0.910 sec/batch)
2017-04-07 21:33:25.087108: step 4860, loss = 20.70167 (5.5 examples/sec; 0.908 sec/batch)
2017-04-07 21:33:34.219492: step 4870, loss = 20.68221 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 21:33:43.270168: step 4880, loss = 21.08648 (5.5 examples/sec; 0.905 sec/batch)
2017-04-07 21:33:52.351324: step 4890, loss = 20.78800 (5.5 examples/sec; 0.908 sec/batch)
2017-04-07 21:34:02.466535: step 4900, loss = 20.49824 (4.9 examples/sec; 1.012 sec/batch)
2017-04-07 21:34:11.596006: step 4910, loss = 20.61705 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 21:34:20.764211: step 4920, loss = 20.73922 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 21:34:29.862351: step 4930, loss = 20.86264 (5.5 examples/sec; 0.910 sec/batch)
2017-04-07 21:34:39.025850: step 4940, loss = 20.56866 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 21:34:48.227032: step 4950, loss = 20.69002 (5.4 examples/sec; 0.920 sec/batch)
2017-04-07 21:34:57.345301: step 4960, loss = 20.39621 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 21:35:06.416461: step 4970, loss = 20.51878 (5.5 examples/sec; 0.907 sec/batch)
2017-04-07 21:35:15.535549: step 4980, loss = 20.64096 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 21:35:24.689611: step 4990, loss = 20.34736 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 21:35:34.830048: step 5000, loss = 20.60854 (4.9 examples/sec; 1.014 sec/batch)
2017-04-07 21:35:43.886058: step 5010, loss = 20.45344 (5.5 examples/sec; 0.906 sec/batch)
2017-04-07 21:35:53.039753: step 5020, loss = 20.30173 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 21:36:02.158598: step 5030, loss = 20.28532 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 21:36:11.249554: step 5040, loss = 20.40484 (5.5 examples/sec; 0.909 sec/batch)
2017-04-07 21:36:20.429454: step 5050, loss = 20.38895 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 21:36:29.553229: step 5060, loss = 20.37420 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 21:36:38.715145: step 5070, loss = 20.91070 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 21:36:47.754662: step 5080, loss = 20.34005 (5.5 examples/sec; 0.904 sec/batch)
2017-04-07 21:36:56.879402: step 5090, loss = 20.73976 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 21:37:07.062888: step 5100, loss = 20.30779 (4.9 examples/sec; 1.018 sec/batch)
2017-04-07 21:37:16.164888: step 5110, loss = 20.29307 (5.5 examples/sec; 0.910 sec/batch)
2017-04-07 21:37:25.309092: step 5120, loss = 20.13689 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 21:37:34.404858: step 5130, loss = 20.26303 (5.5 examples/sec; 0.910 sec/batch)
2017-04-07 21:37:43.576999: step 5140, loss = 20.24384 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 21:37:52.723858: step 5150, loss = 20.22756 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 21:38:01.846067: step 5160, loss = 20.34984 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 21:38:10.956368: step 5170, loss = 20.61104 (5.5 examples/sec; 0.911 sec/batch)
2017-04-07 21:38:20.025036: step 5180, loss = 20.17916 (5.5 examples/sec; 0.907 sec/batch)
2017-04-07 21:38:29.136678: step 5190, loss = 20.30221 (5.5 examples/sec; 0.911 sec/batch)
2017-04-07 21:38:39.807025: step 5200, loss = 20.28571 (4.7 examples/sec; 1.067 sec/batch)
2017-04-07 21:38:48.882441: step 5210, loss = 19.99316 (5.5 examples/sec; 0.908 sec/batch)
2017-04-07 21:38:58.099227: step 5220, loss = 19.97650 (5.4 examples/sec; 0.922 sec/batch)
2017-04-07 21:39:07.203483: step 5230, loss = 20.09915 (5.5 examples/sec; 0.910 sec/batch)
2017-04-07 21:39:16.325500: step 5240, loss = 20.08316 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 21:39:25.459093: step 5250, loss = 19.92864 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 21:39:34.590979: step 5260, loss = 20.18995 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 21:39:43.730107: step 5270, loss = 19.89732 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 21:39:52.841141: step 5280, loss = 20.15822 (5.5 examples/sec; 0.911 sec/batch)
2017-04-07 21:40:01.990196: step 5290, loss = 20.00370 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 21:40:12.182761: step 5300, loss = 20.12041 (4.9 examples/sec; 1.019 sec/batch)
2017-04-07 21:40:21.346860: step 5310, loss = 19.83322 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 21:40:30.543105: step 5320, loss = 19.95616 (5.4 examples/sec; 0.920 sec/batch)
2017-04-07 21:40:39.720759: step 5330, loss = 19.94011 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 21:40:48.890882: step 5340, loss = 19.92435 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 21:40:58.019042: step 5350, loss = 19.77564 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 21:41:07.197676: step 5360, loss = 19.89300 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 21:41:16.302818: step 5370, loss = 20.01583 (5.5 examples/sec; 0.911 sec/batch)
2017-04-07 21:41:25.412478: step 5380, loss = 19.72745 (5.5 examples/sec; 0.911 sec/batch)
2017-04-07 21:41:34.615490: step 5390, loss = 19.98483 (5.4 examples/sec; 0.920 sec/batch)
2017-04-07 21:41:44.636901: step 5400, loss = 20.76102 (5.0 examples/sec; 1.002 sec/batch)
2017-04-07 21:41:53.767315: step 5410, loss = 19.95940 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 21:42:02.913747: step 5420, loss = 19.95090 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 21:42:12.080208: step 5430, loss = 19.66468 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 21:42:21.221752: step 5440, loss = 20.04519 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 21:42:30.313026: step 5450, loss = 19.77522 (5.5 examples/sec; 0.909 sec/batch)
2017-04-07 21:42:39.361635: step 5460, loss = 19.60943 (5.5 examples/sec; 0.905 sec/batch)
2017-04-07 21:42:48.543953: step 5470, loss = 21.16611 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 21:42:57.685250: step 5480, loss = 19.72082 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 21:43:06.841163: step 5490, loss = 19.84378 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 21:43:16.805976: step 5500, loss = 19.60427 (5.0 examples/sec; 0.996 sec/batch)
2017-04-07 21:43:25.966258: step 5510, loss = 19.66197 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 21:43:35.170985: step 5520, loss = 19.64972 (5.4 examples/sec; 0.920 sec/batch)
2017-04-07 21:43:44.344519: step 5530, loss = 19.64482 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 21:43:53.426580: step 5540, loss = 19.77541 (5.5 examples/sec; 0.908 sec/batch)
2017-04-07 21:44:02.501273: step 5550, loss = 19.74002 (5.5 examples/sec; 0.907 sec/batch)
2017-04-07 21:44:11.634058: step 5560, loss = 19.58239 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 21:44:20.757576: step 5570, loss = 19.42846 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 21:44:29.872252: step 5580, loss = 19.57985 (5.5 examples/sec; 0.911 sec/batch)
2017-04-07 21:44:38.989074: step 5590, loss = 19.53688 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 21:44:48.998787: step 5600, loss = 19.52064 (5.0 examples/sec; 1.001 sec/batch)
2017-04-07 21:44:58.128654: step 5610, loss = 19.37035 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 21:45:07.338530: step 5620, loss = 19.48945 (5.4 examples/sec; 0.921 sec/batch)
2017-04-07 21:45:16.530902: step 5630, loss = 19.49259 (5.4 examples/sec; 0.919 sec/batch)
2017-04-07 21:45:25.686234: step 5640, loss = 19.45862 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 21:45:34.887392: step 5650, loss = 19.44679 (5.4 examples/sec; 0.920 sec/batch)
2017-04-07 21:45:44.058973: step 5660, loss = 19.43519 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 21:45:53.183359: step 5670, loss = 19.27380 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 21:46:02.343728: step 5680, loss = 19.25929 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 21:46:11.490436: step 5690, loss = 19.38733 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 21:46:21.522468: step 5700, loss = 19.50468 (5.0 examples/sec; 1.003 sec/batch)
2017-04-07 21:46:30.654091: step 5710, loss = 19.35197 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 21:46:39.804597: step 5720, loss = 19.33780 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 21:46:49.004094: step 5730, loss = 19.32044 (5.4 examples/sec; 0.920 sec/batch)
2017-04-07 21:46:58.126187: step 5740, loss = 19.35311 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 21:47:07.280990: step 5750, loss = 19.15105 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 21:47:16.407259: step 5760, loss = 19.13947 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 21:47:25.558057: step 5770, loss = 19.12014 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 21:47:34.769225: step 5780, loss = 19.38207 (5.4 examples/sec; 0.921 sec/batch)
2017-04-07 21:47:43.861416: step 5790, loss = 19.22851 (5.5 examples/sec; 0.909 sec/batch)
2017-04-07 21:47:53.997305: step 5800, loss = 19.34327 (4.9 examples/sec; 1.014 sec/batch)
2017-04-07 21:48:03.146739: step 5810, loss = 19.33691 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 21:48:12.327308: step 5820, loss = 19.04439 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 21:48:21.471413: step 5830, loss = 19.16733 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 21:48:31.192125: step 5840, loss = 19.42936 (5.1 examples/sec; 0.972 sec/batch)
2017-04-07 21:48:40.362791: step 5850, loss = 19.27552 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 21:48:49.508852: step 5860, loss = 19.26034 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 21:48:58.618171: step 5870, loss = 19.12740 (5.5 examples/sec; 0.911 sec/batch)
2017-04-07 21:49:07.809493: step 5880, loss = 19.23077 (5.4 examples/sec; 0.919 sec/batch)
2017-04-07 21:49:17.001419: step 5890, loss = 19.21486 (5.4 examples/sec; 0.919 sec/batch)
2017-04-07 21:49:27.136476: step 5900, loss = 19.06150 (4.9 examples/sec; 1.014 sec/batch)
2017-04-07 21:49:36.193487: step 5910, loss = 19.32333 (5.5 examples/sec; 0.906 sec/batch)
2017-04-07 21:49:45.324413: step 5920, loss = 19.03085 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 21:49:54.451611: step 5930, loss = 19.02225 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 21:50:03.635447: step 5940, loss = 19.00080 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 21:50:12.796625: step 5950, loss = 19.12421 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 21:50:21.971866: step 5960, loss = 18.83196 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 21:50:31.095266: step 5970, loss = 19.09424 (5.5 examples/sec; 0.912 sec/batch)
2017-04-07 21:50:40.227606: step 5980, loss = 18.94039 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 21:50:49.281333: step 5990, loss = 18.93762 (5.5 examples/sec; 0.905 sec/batch)
2017-04-07 21:50:59.233272: step 6000, loss = 19.04904 (5.0 examples/sec; 0.995 sec/batch)
2017-04-07 21:51:08.374074: step 6010, loss = 18.75692 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 21:51:17.531589: step 6020, loss = 19.15766 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 21:51:26.687226: step 6030, loss = 19.00412 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 21:51:35.892011: step 6040, loss = 18.85203 (5.4 examples/sec; 0.920 sec/batch)
2017-04-07 21:51:45.043739: step 6050, loss = 18.83546 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 21:51:54.245699: step 6060, loss = 18.82048 (5.4 examples/sec; 0.920 sec/batch)
2017-04-07 21:52:03.379386: step 6070, loss = 18.66699 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 21:52:12.565423: step 6080, loss = 18.65220 (5.4 examples/sec; 0.919 sec/batch)
2017-04-07 21:52:21.751261: step 6090, loss = 18.63709 (5.4 examples/sec; 0.919 sec/batch)
2017-04-07 21:52:31.901934: step 6100, loss = 18.76085 (4.9 examples/sec; 1.015 sec/batch)
2017-04-07 21:52:41.084151: step 6110, loss = 18.75001 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 21:52:50.257590: step 6120, loss = 18.73231 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 21:52:59.473057: step 6130, loss = 18.71635 (5.4 examples/sec; 0.922 sec/batch)
2017-04-07 21:53:08.635198: step 6140, loss = 18.70134 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 21:53:17.862157: step 6150, loss = 18.82609 (5.4 examples/sec; 0.923 sec/batch)
2017-04-07 21:53:27.024171: step 6160, loss = 18.81114 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 21:53:36.200345: step 6170, loss = 18.80864 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 21:53:45.365437: step 6180, loss = 18.78073 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 21:53:54.564414: step 6190, loss = 18.76597 (5.4 examples/sec; 0.920 sec/batch)
2017-04-07 21:54:04.611802: step 6200, loss = 18.75325 (5.0 examples/sec; 1.005 sec/batch)
2017-04-07 21:54:13.763215: step 6210, loss = 18.59772 (5.5 examples/sec; 0.915 sec/batch)
2017-04-07 21:54:22.963481: step 6220, loss = 18.44436 (5.4 examples/sec; 0.920 sec/batch)
2017-04-07 21:54:32.133405: step 6230, loss = 18.70680 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 21:54:41.331186: step 6240, loss = 18.55348 (5.4 examples/sec; 0.920 sec/batch)
2017-04-07 21:54:50.466211: step 6250, loss = 18.40009 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 21:54:59.594281: step 6260, loss = 18.52406 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 21:55:08.755371: step 6270, loss = 18.37099 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 21:55:17.894269: step 6280, loss = 18.64816 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 21:55:27.064826: step 6290, loss = 18.75720 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 21:55:37.838922: step 6300, loss = 18.46528 (4.6 examples/sec; 1.077 sec/batch)
2017-04-07 21:56:09.139083: step 6310, loss = 18.45387 (1.6 examples/sec; 3.130 sec/batch)
2017-04-07 21:56:34.578192: step 6320, loss = 18.57463 (2.0 examples/sec; 2.544 sec/batch)
2017-04-07 21:56:43.806663: step 6330, loss = 18.69883 (5.4 examples/sec; 0.923 sec/batch)
2017-04-07 21:56:53.974006: step 6340, loss = 18.40677 (4.9 examples/sec; 1.017 sec/batch)
2017-04-07 21:57:03.309523: step 6350, loss = 18.39283 (5.4 examples/sec; 0.934 sec/batch)
2017-04-07 21:57:12.552331: step 6360, loss = 18.37777 (5.4 examples/sec; 0.924 sec/batch)
2017-04-07 21:57:21.852703: step 6370, loss = 18.36299 (5.4 examples/sec; 0.930 sec/batch)
2017-04-07 21:57:31.188444: step 6380, loss = 18.20993 (5.4 examples/sec; 0.934 sec/batch)
2017-04-07 21:57:40.441951: step 6390, loss = 18.33385 (5.4 examples/sec; 0.925 sec/batch)
2017-04-07 21:57:50.675274: step 6400, loss = 18.33688 (4.9 examples/sec; 1.023 sec/batch)
2017-04-07 21:57:59.922698: step 6410, loss = 18.31059 (5.4 examples/sec; 0.925 sec/batch)
2017-04-07 21:58:09.090726: step 6420, loss = 18.15170 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 21:58:18.433535: step 6430, loss = 18.14283 (5.4 examples/sec; 0.934 sec/batch)
2017-04-07 21:58:27.733406: step 6440, loss = 18.26133 (5.4 examples/sec; 0.930 sec/batch)
2017-04-07 21:58:37.575542: step 6450, loss = 18.38543 (5.1 examples/sec; 0.984 sec/batch)
2017-04-07 21:58:46.861569: step 6460, loss = 18.09373 (5.4 examples/sec; 0.929 sec/batch)
2017-04-07 21:58:56.020668: step 6470, loss = 18.21790 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 21:59:05.178260: step 6480, loss = 18.20376 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 21:59:14.459411: step 6490, loss = 18.18899 (5.4 examples/sec; 0.928 sec/batch)
2017-04-07 21:59:24.541802: step 6500, loss = 18.17457 (5.0 examples/sec; 1.008 sec/batch)
2017-04-07 21:59:33.792786: step 6510, loss = 18.16400 (5.4 examples/sec; 0.925 sec/batch)
2017-04-07 21:59:43.016240: step 6520, loss = 18.15448 (5.4 examples/sec; 0.922 sec/batch)
2017-04-07 21:59:52.186581: step 6530, loss = 17.99320 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 22:00:01.414183: step 6540, loss = 17.97901 (5.4 examples/sec; 0.923 sec/batch)
2017-04-07 22:00:10.668015: step 6550, loss = 18.37986 (5.4 examples/sec; 0.925 sec/batch)
2017-04-07 22:00:19.936147: step 6560, loss = 18.08822 (5.4 examples/sec; 0.927 sec/batch)
2017-04-07 22:00:29.198776: step 6570, loss = 18.21249 (5.4 examples/sec; 0.926 sec/batch)
2017-04-07 22:00:38.379104: step 6580, loss = 18.33680 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 22:00:47.687056: step 6590, loss = 18.04517 (5.4 examples/sec; 0.931 sec/batch)
2017-04-07 22:00:58.010460: step 6600, loss = 18.16953 (4.8 examples/sec; 1.032 sec/batch)
2017-04-07 22:01:07.174188: step 6610, loss = 17.87863 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 22:01:16.449167: step 6620, loss = 18.00342 (5.4 examples/sec; 0.927 sec/batch)
2017-04-07 22:01:25.705858: step 6630, loss = 17.85159 (5.4 examples/sec; 0.926 sec/batch)
2017-04-07 22:01:34.985857: step 6640, loss = 17.97667 (5.4 examples/sec; 0.928 sec/batch)
2017-04-07 22:01:44.299355: step 6650, loss = 17.96247 (5.4 examples/sec; 0.931 sec/batch)
2017-04-07 22:01:53.533224: step 6660, loss = 17.94865 (5.4 examples/sec; 0.923 sec/batch)
2017-04-07 22:02:02.705813: step 6670, loss = 17.97680 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 22:02:12.026228: step 6680, loss = 17.92132 (5.4 examples/sec; 0.932 sec/batch)
2017-04-07 22:02:21.298801: step 6690, loss = 17.76904 (5.4 examples/sec; 0.927 sec/batch)
2017-04-07 22:02:31.483798: step 6700, loss = 18.03267 (4.9 examples/sec; 1.018 sec/batch)
2017-04-07 22:02:40.666275: step 6710, loss = 17.88053 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 22:02:49.921494: step 6720, loss = 18.00542 (5.4 examples/sec; 0.926 sec/batch)
2017-04-07 22:02:59.157778: step 6730, loss = 17.71474 (5.4 examples/sec; 0.924 sec/batch)
2017-04-07 22:03:08.331464: step 6740, loss = 17.84016 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 22:03:17.569937: step 6750, loss = 17.68733 (5.4 examples/sec; 0.924 sec/batch)
2017-04-07 22:03:26.799857: step 6760, loss = 17.81239 (5.4 examples/sec; 0.923 sec/batch)
2017-04-07 22:03:36.062287: step 6770, loss = 17.79883 (5.4 examples/sec; 0.926 sec/batch)
2017-04-07 22:03:45.393800: step 6780, loss = 17.64664 (5.4 examples/sec; 0.933 sec/batch)
2017-04-07 22:03:54.661604: step 6790, loss = 18.04931 (5.4 examples/sec; 0.927 sec/batch)
2017-04-07 22:04:04.902079: step 6800, loss = 17.76763 (4.9 examples/sec; 1.024 sec/batch)
2017-04-07 22:04:14.087541: step 6810, loss = 17.74473 (5.4 examples/sec; 0.919 sec/batch)
2017-04-07 22:04:23.383151: step 6820, loss = 17.59255 (5.4 examples/sec; 0.930 sec/batch)
2017-04-07 22:04:32.671997: step 6830, loss = 17.71765 (5.4 examples/sec; 0.929 sec/batch)
2017-04-07 22:04:41.902755: step 6840, loss = 17.84293 (5.4 examples/sec; 0.923 sec/batch)
2017-04-07 22:04:51.116142: step 6850, loss = 17.69069 (5.4 examples/sec; 0.921 sec/batch)
2017-04-07 22:05:00.333277: step 6860, loss = 17.81584 (5.4 examples/sec; 0.922 sec/batch)
2017-04-07 22:05:09.609018: step 6870, loss = 17.66373 (5.4 examples/sec; 0.928 sec/batch)
2017-04-07 22:05:18.960422: step 6880, loss = 17.65040 (5.3 examples/sec; 0.935 sec/batch)
2017-04-07 22:05:28.098405: step 6890, loss = 17.63685 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 22:05:38.310503: step 6900, loss = 17.62342 (4.9 examples/sec; 1.021 sec/batch)
2017-04-07 22:05:47.498650: step 6910, loss = 17.74867 (5.4 examples/sec; 0.919 sec/batch)
2017-04-07 22:05:56.717056: step 6920, loss = 17.45795 (5.4 examples/sec; 0.922 sec/batch)
2017-04-07 22:06:05.962078: step 6930, loss = 17.58317 (5.4 examples/sec; 0.925 sec/batch)
2017-04-07 22:06:15.280353: step 6940, loss = 17.70880 (5.4 examples/sec; 0.932 sec/batch)
2017-04-07 22:06:24.515175: step 6950, loss = 17.55640 (5.4 examples/sec; 0.923 sec/batch)
2017-04-07 22:06:33.782131: step 6960, loss = 17.68242 (5.4 examples/sec; 0.927 sec/batch)
2017-04-07 22:06:43.007889: step 6970, loss = 17.39108 (5.4 examples/sec; 0.923 sec/batch)
2017-04-07 22:06:52.233517: step 6980, loss = 17.51637 (5.4 examples/sec; 0.923 sec/batch)
2017-04-07 22:07:01.484169: step 6990, loss = 17.50298 (5.4 examples/sec; 0.925 sec/batch)
2017-04-07 22:07:11.612008: step 7000, loss = 17.76693 (4.9 examples/sec; 1.013 sec/batch)
2017-04-07 22:07:20.718314: step 7010, loss = 17.75359 (5.5 examples/sec; 0.911 sec/batch)
2017-04-07 22:07:30.064313: step 7020, loss = 17.32440 (5.3 examples/sec; 0.935 sec/batch)
2017-04-07 22:07:39.378116: step 7030, loss = 17.45035 (5.4 examples/sec; 0.931 sec/batch)
2017-04-07 22:07:48.581819: step 7040, loss = 17.43648 (5.4 examples/sec; 0.920 sec/batch)
2017-04-07 22:07:57.772445: step 7050, loss = 17.56277 (5.4 examples/sec; 0.919 sec/batch)
2017-04-07 22:08:06.932335: step 7060, loss = 17.54853 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 22:08:16.240893: step 7070, loss = 17.53526 (5.4 examples/sec; 0.931 sec/batch)
2017-04-07 22:08:25.512040: step 7080, loss = 17.38372 (5.4 examples/sec; 0.927 sec/batch)
2017-04-07 22:08:35.269835: step 7090, loss = 17.37018 (5.1 examples/sec; 0.976 sec/batch)
2017-04-07 22:08:45.523848: step 7100, loss = 17.49562 (4.9 examples/sec; 1.025 sec/batch)
2017-04-07 22:08:54.716678: step 7110, loss = 17.62099 (5.4 examples/sec; 0.919 sec/batch)
2017-04-07 22:09:03.943196: step 7120, loss = 17.60777 (5.4 examples/sec; 0.923 sec/batch)
2017-04-07 22:09:13.111902: step 7130, loss = 17.45594 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 22:09:22.293604: step 7140, loss = 17.44274 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 22:09:31.516188: step 7150, loss = 17.29096 (5.4 examples/sec; 0.922 sec/batch)
2017-04-07 22:09:40.786040: step 7160, loss = 17.13922 (5.4 examples/sec; 0.927 sec/batch)
2017-04-07 22:09:50.068113: step 7170, loss = 17.26463 (5.4 examples/sec; 0.928 sec/batch)
2017-04-07 22:09:59.445302: step 7180, loss = 17.39009 (5.3 examples/sec; 0.938 sec/batch)
2017-04-07 22:10:08.613998: step 7190, loss = 17.23835 (5.5 examples/sec; 0.917 sec/batch)
2017-04-07 22:10:18.527231: step 7200, loss = 17.36382 (5.0 examples/sec; 0.991 sec/batch)
2017-04-07 22:10:27.642171: step 7210, loss = 17.21212 (5.5 examples/sec; 0.911 sec/batch)
2017-04-07 22:10:36.744690: step 7220, loss = 17.47722 (5.5 examples/sec; 0.910 sec/batch)
2017-04-07 22:10:45.798008: step 7230, loss = 17.05710 (5.5 examples/sec; 0.905 sec/batch)
2017-04-07 22:10:54.981870: step 7240, loss = 17.31152 (5.4 examples/sec; 0.918 sec/batch)
2017-04-07 22:11:04.138969: step 7250, loss = 17.15973 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 22:11:13.283954: step 7260, loss = 17.00806 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 22:11:22.420408: step 7270, loss = 16.99498 (5.5 examples/sec; 0.914 sec/batch)
2017-04-07 22:11:31.446439: step 7280, loss = 17.12175 (5.5 examples/sec; 0.903 sec/batch)
2017-04-07 22:11:40.546369: step 7290, loss = 17.10786 (5.5 examples/sec; 0.910 sec/batch)
2017-04-07 22:11:50.395593: step 7300, loss = 17.09490 (5.1 examples/sec; 0.985 sec/batch)
2017-04-07 22:11:59.456079: step 7310, loss = 16.94541 (5.5 examples/sec; 0.906 sec/batch)
2017-04-07 22:12:08.613903: step 7320, loss = 17.06846 (5.5 examples/sec; 0.916 sec/batch)
2017-04-07 22:12:17.748433: step 7330, loss = 17.11860 (5.5 examples/sec; 0.913 sec/batch)
2017-04-07 22:12:26.796417: step 7340, loss = 17.04251 (5.5 examples/sec; 0.905 sec/batch)
2017-04-07 22:12:36.128522: step 7350, loss = 17.44543 (5.4 examples/sec; 0.933 sec/batch)
2017-04-07 22:12:45.180384: step 7360, loss = 17.01665 (5.5 examples/sec; 0.905 sec/batch)
2017-04-07 22:12:54.404320: step 7370, loss = 17.00873 (5.4 examples/sec; 0.922 sec/batch)
